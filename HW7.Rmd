---
title: "HW7: TEAM  [my team number/name here]"
author: "[my team members here]"
date: "Due Date in Sakai"
output: html_notebook
---

1. (individual)  Complete Exercise 7 in ISLR

2. (individual) For $p=2$ create a plot showing the contours of the log likelihood surface  and contours of the log density of $\beta_1$ and $\beta_2$ for the case of independent Cauchy priors   (Student t densities with 1 degree of freedom).  For the same likelihood, make a plots for  the Lasso or Ridge priors as Figure 6.7   Comment on whether you think this will lead to variable selection under the MAP estimate.

3.  Refer to the College data that we used previously.  In the code below modify the seed for the random number to be your Team Number.
```{r}
library(ISLR)
data(College)
set.seed(0)
```

    a. split the data into a 75% training set and a 25% test set.
    
    b. Using the recommended transformations from previous HW for the normal regression model with your  most complex model (no variable selection), obtain the RMSE for predicting the number of applications (not the transformed response) for the test data under OLS.
    
    c. Using the same variables as above,  obtain the RMSE for the test data using ridge regression with $\lambda$ chosen by cross-validation (the cross-validation for choosing $\lambda$ should only use the training data).
    
    d. Using the same variables as above, obtain the RMSE for the test data using lasso with $\lambda$ chosen by cross-validation.  Report on which variables are selected by lasso.
    
    e. (optional)  Using the same variables, obtain the RMSE for the test data using one of the mixtures of $g$ priors under BMA  Report on  which variables are viewed as important under the BMA model.

    f.  Using the same variables, obtain the RMSE for the test data using the horseshoe prior, using  `bhs` in `library(monomvn)`,  with`RJ=FALSE`.    Report on  which variables are viewed as important under the  horseshoe with variable selection.
    
    g.  For the above methods that can produce prediction intervals, determine what percent of the test observations are included inside 95% prediction intervals and report a table of coverage values and comment on results.   For example, if `lm.ridge` does not provide prediction intervals, can use the blasso function to obtain prediction intervals?
    
    h.  (plan ahead for this and ask questions Friday)   Use Student $t$ errors 8 degrees of freedom and a prior that has heavier tails than the error distribution  modify the JAGS  code below to fit a model to the training data. Report the RMSE and coverage on the test data.

```{r}
library(R2jags)

model = function() {
  for (i in 1:n) {
    Y[i] ~ dnorm(mu[i], phi*lambda[i])  # note second argument is a precision
    lambda[i] ~ gamma(8/2, 8/2) # this leads to a St with 8 degrees of freedom if we integrate out lambda
    mu[i] = inprod(x[i,], beta) # mean function
  }
  # priors here
}
```
    

    
4. (Plan ahead to ask questions Friday)  For the college data, the negative binomial model seemed to provide the best model.   Using the representation of the Negative Binomial as a gamma mixture of Poissons (HW4),   modify the JAGS code so that the response has a Poisson distribution with mean `lambda[i]` and that `lambda[i]` has a gamma distriubtion as in problem 20 of HW4.   Using scaled predictors, implement one of the scale mixtures of normal priors (lasso  horseshoe, or other) in JAGS. Using JAGS to obtain predictions, report the RMSE and coverage of credible intervals for the test data.  


5.  Provide a summary paragraph comment on the results obtained in 3 and 4.  How accurately can you predict the number of applications?  Is there much difference in RMSEs among these methods? Is there much difference in coverage among these methods? 

6. For your recommended model, provide CI for all of the parameters, and pick 5 (or more) of the most important variables and provide a paragraph that provides an interpretation of the parameters (and intervals) that can be provided to a university admissions officer about which variables increase admissions. 

    
    

   

